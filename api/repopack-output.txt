This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-23T02:11:25.644Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
app.py
Dockerfile
llm.py
requirements.txt

================================================================
Repository Files
================================================================

================
File: .gitignore
================
__pycache__/
*.py[cod]
*$py.class
last_response.txt

================
File: app.py
================
from flask import Flask, jsonify, request
from llm import request_code

app = Flask(__name__)

@app.route('/')
def home():
    print("test")
    return "Hello. Home API Endpoint"

@app.route('/api/imgtocode', methods=['POST'])
def imgtocode():
    json = request.json
    image_url = json['image_url']
    code = request_code(image_url)
    response = {
        'code': code
    }
    return jsonify(response), 201

================
File: Dockerfile
================
FROM python:3.8-slim-buster

# Set working directory
WORKDIR /app

# Copy only requirements first to leverage Docker cache
COPY requirements.txt requirements.txt

# Install dependencies
RUN pip install -r requirements.txt

# Create a non-root user and switch to it
RUN adduser --disabled-password --gecos '' appuser

# Copy the rest of the application
COPY . .

# Set proper permissions for the application directory
RUN chown -R appuser:appuser /app

# The API key should be passed at runtime, not build time
ENV OPENAI_API_KEY="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"


# Set Python to run in unbuffered mode
ENV PYTHONUNBUFFERED=1

# Switch to non-root user
USER appuser

# Command to run the application with error logging
CMD ["python", "-u", "app.py"]

================
File: llm.py
================
import sys
from openai import OpenAI

client = OpenAI()

def request_code(image_url):
    instructions = "Please transcribe the content of this image into code. Return only the name of the coding language used, bolded in markdown, and one markdown code box with the extracted code. The markdown code box should be language ambiguious, denoted using only triple backticks."

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": instructions},
                {"type": "image_url", "image_url": { "url":  image_url }}
            ]
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=1000,
    )

    if response.choices:
        with open('last_response.txt', 'w') as f:
            f.write(str(response.choices[0].message.content))
        code = response.choices[0].message.content.split("```")[1]
        language = response.choices[0].message.content.split("**")[1]
        return code, language
    else:
        print("Error: No response received from the model.")
        sys.exit(1)

================
File: requirements.txt
================
Flask>=2.2.2
openai>=1.54.5
